import argparse
import csv
from pathlib import Path
import numpy as np
import pandas as pd
import librosa
import cv2


def analyze_track(path: Path):
    try:
        y, sr = librosa.load(path, sr=None, mono=True)
        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
        rms = float(np.mean(librosa.feature.rms(y=y)))
        centroid = float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))
        return {"tempo": float(tempo), "rms": rms, "centroid": centroid}
    except Exception:
        return {"tempo": None, "rms": None, "centroid": None}


def image_features(img_path: str):
    try:
        img = cv2.imread(img_path)
        if img is None:
            return None
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        h, s, v = cv2.split(hsv)
        brightness = float(np.mean(v) / 255.0)
        saturation = float(np.mean(s) / 255.0)

        # Colorfulness (Hasler-Susstrunk)
        r, g, b = img[:,:,0].astype(np.float32), img[:,:,1].astype(np.float32), img[:,:,2].astype(np.float32)
        rg = r - g
        yb = 0.5 * (r + g) - b
        std_rg, std_yb = np.std(rg), np.std(yb)
        mean_rg, mean_yb = np.mean(rg), np.mean(yb)
        colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)
        # Normalize colorfulness roughly to [0,1] using a heuristic scale
        colorfulness_n = float(np.clip(colorfulness / 100.0, 0.0, 1.0))

        # Edge density via Sobel gradients
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
        mag = np.sqrt(gx*gx + gy*gy)
        edges = (mag > np.percentile(mag, 75)).astype(np.float32)
        edge_density = float(edges.mean())

        # Dominant hue (circular mean)
        hue = (h.astype(np.float32) / 180.0) * 2 * np.pi
        mean_sin, mean_cos = np.mean(np.sin(hue)), np.mean(np.cos(hue))
        dom_hue = float((np.arctan2(mean_sin, mean_cos) % (2*np.pi)) / (2*np.pi))  # [0,1]

        return {
            "brightness": brightness,
            "saturation": saturation,
            "colorfulness": colorfulness_n,
            "edge_density": edge_density,
            "dominant_hue": dom_hue,
        }
    except Exception:
        return None


def aggregate_cluster_features(index_csv: Path, labels_csv: Path):
    idx = pd.read_csv(index_csv)
    labels = pd.read_csv(labels_csv)
    # Align by row order: index.csv generated by ingest is in same order as embeddings/labels
    idx = idx.reset_index(drop=True)
    labels = labels.reset_index(drop=True)
    assert len(idx) == len(labels), "Index and labels length mismatch"

    feats_by_cluster = {}
    for lab in sorted(set(labels["label"])):
        if lab == -1:
            continue
        rows = idx[labels["label"] == lab]
        # Sample up to 50 images per cluster for speed
        paths = rows["path"].tolist()[:50]
        feat_list = [image_features(p) for p in paths]
        feat_list = [f for f in feat_list if f is not None]
        if not feat_list:
            continue
        agg = {
            k: float(np.mean([f[k] for f in feat_list])) for k in feat_list[0].keys()
        }
        feats_by_cluster[int(lab)] = agg
    return feats_by_cluster


def score_track_for_cluster(track_stats, cluster_feat):
    # Desired attributes derived from visuals
    target_bpm = 80 + 60*cluster_feat["saturation"] + 40*cluster_feat["edge_density"]  # 80–180 approx
    target_centroid = 1500 + 3000*cluster_feat["brightness"]  # 1.5–4.5 kHz
    target_energy = 0.02 + 0.08*(0.6*cluster_feat["saturation"] + 0.4*cluster_feat["edge_density"])  # rough RMS

    tempo = track_stats.get("tempo") or target_bpm
    centroid = track_stats.get("centroid") or target_centroid
    rms = track_stats.get("rms") or target_energy

    # Normalize distances
    bpm_score = np.exp(-((tempo - target_bpm)/25.0)**2)
    cent_score = np.exp(-((centroid - target_centroid)/1200.0)**2)
    energy_score = np.exp(-((rms - target_energy)/0.03)**2)

    # Weighted sum
    return 0.45*bpm_score + 0.35*cent_score + 0.20*energy_score


def main():
    parser = argparse.ArgumentParser(description="Select background music per cluster based on visual attributes")
    parser.add_argument("--tracks", required=True, help="Folder with audio tracks (wav/mp3)")
    parser.add_argument("--index", required=True, help="Index CSV produced by ingest.py")
    parser.add_argument("--clusters", required=True, help="CSV with labels (column 'label')")
    parser.add_argument("--out", required=True, help="Output CSV plan")
    args = parser.parse_args()

    # Analyze available tracks
    track_paths = sorted(Path(args.tracks).glob("*.mp3")) + sorted(Path(args.tracks).glob("*.wav"))
    if not track_paths:
        print("No audio tracks found; cannot build music plan.")
        return
    track_stats = {str(p): analyze_track(p) for p in track_paths}

    # Compute cluster visual features
    cluster_feats = aggregate_cluster_features(Path(args.index), Path(args.clusters))

    # Build plan
    rows = []
    for cid, feat in cluster_feats.items():
        # Score all tracks and pick best
        best_track = None
        best_score = -1.0
        for tp, st in track_stats.items():
            sc = score_track_for_cluster(st, feat)
            if sc > best_score:
                best_score, best_track = sc, tp
        rows.append({"cluster": int(cid), "track": best_track})

    out = Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)
    with out.open("w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["cluster", "track"]) 
        writer.writeheader()
        writer.writerows(rows)
    print(f"Wrote music plan for {len(rows)} clusters -> {out}")

if __name__ == "__main__":
    main()
